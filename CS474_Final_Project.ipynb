{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K3TVIHAQJl8E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c010d38-2310-4448-bc2a-d6921ffaab72"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting unidecode\n",
            "  Downloading Unidecode-1.3.8-py3-none-any.whl (235 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/235.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━\u001b[0m \u001b[32m225.3/235.5 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.5/235.5 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: unidecode\n",
            "Successfully installed unidecode-1.3.8\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.2.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch)\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105\n"
          ]
        }
      ],
      "source": [
        "# Heres where I upload the data to a dataframe and do a lot of imports\n",
        "! pip install unidecode\n",
        "! pip install torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from collections import Counter\n",
        "import unidecode\n",
        "\n",
        "class PickupLinesDataset(Dataset):\n",
        "  def __init__(self, data, chunk_len):\n",
        "    self.data = data\n",
        "    self.chunk_len = chunk_len\n",
        "    self.chars = tuple(set(data))\n",
        "    self.char_to_int = {ch: i for i, ch in enumerate(self.chars)}\n",
        "    self.int_to_char = dict(enumerate(self.chars))\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data) - self.chunk_len\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    return (\n",
        "        torch.tensor([self.char_to_int[ch] for ch in self.data[index:index+self.chunk_len]]),\n",
        "        torch.tensor([self.char_to_int[self.data[index+self.chunk_len]]])\n",
        "    )\n",
        "\n",
        "\n",
        "url = 'https://raw.githubusercontent.com/jacobjones36/pickuplines/main/lines_revised.csv?token=GHSAT0AAAAAACMV2YC3Z7ARNOE3BZ3NWVYYZRHAEDQ'\n",
        "df = pd.read_csv(url)\n",
        "#print(df['Normal'][1000])\n",
        "normal = df['Normal'].astype(str).str.strip().replace('nan', pd.NA).dropna().tolist()\n",
        "knock = df['Knock'].astype(str).str.strip().replace('nan', pd.NA).dropna().tolist()\n",
        "math = df['Math'].astype(str).str.strip().replace('nan', pd.NA).dropna().tolist()\n",
        "science = df['Science'].astype(str).str.strip().replace('nan', pd.NA).dropna().tolist()\n",
        "sleep = df['Sleep'].astype(str).str.strip().replace('nan', pd.NA).dropna().tolist()\n",
        "church = df['Church'].astype(str).str.strip().replace('nan', pd.NA).dropna().tolist()\n",
        "#math = '\\n'.join(math)\n",
        "#math = unidecode.unidecode(math)\n",
        "#data = PickupLinesDataset(math, chunk_len=100)\n",
        "#dataloader = DataLoader(dataset, batch_size=128, shuffle=True)"
      ],
      "metadata": {
        "id": "LShE7DjJw2l1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PickupLinesCategoryDataset(Dataset):\n",
        "    def __init__(self, categories, chunk_len):\n",
        "        self.chunk_len = chunk_len\n",
        "        self.data = []\n",
        "        self.vocab = set()\n",
        "\n",
        "        # Initialize category mappings\n",
        "        self.categories = list(categories.keys())\n",
        "        self.cat_to_int = {cat: i for i, cat in enumerate(self.categories)}\n",
        "\n",
        "        # Build vocabulary and prepare data\n",
        "        for category, lines in categories.items():\n",
        "            for line in lines:\n",
        "                self.vocab.update(line)\n",
        "                if len(line) > self.chunk_len:\n",
        "                    for start_idx in range(len(line) - self.chunk_len):\n",
        "                        end_idx = start_idx + self.chunk_len + 1\n",
        "                        input_seq = line[start_idx:end_idx]\n",
        "                        self.data.append((category, input_seq))\n",
        "\n",
        "        self.chars = sorted(list(self.vocab))\n",
        "        self.char_to_int = {ch: i for i, ch in enumerate(self.chars)}\n",
        "        self.int_to_char = {i: ch for i, ch in enumerate(self.chars)}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        category, sequence = self.data[index]\n",
        "        cat_idx = self.cat_to_int[category]\n",
        "        input_seq = torch.tensor([self.char_to_int[ch] for ch in sequence[:-1]], dtype=torch.long)\n",
        "        target_seq = torch.tensor([self.char_to_int[ch] for ch in sequence[1:]], dtype=torch.long)\n",
        "        return cat_idx, input_seq, target_seq\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KKaV9Dc7EOZn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PickupLineCategoryGenerator(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, category_count, category_embedding_dim):\n",
        "        super(PickupLineCategoryGenerator, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.category_embedding = nn.Embedding(category_count, category_embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim + category_embedding_dim, hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, category, seq, hidden=None):\n",
        "        embedded_seq = self.embedding(seq)\n",
        "        embedded_cat = self.category_embedding(category).unsqueeze(1).expand(-1, seq.size(1), -1)\n",
        "        combined_embed = torch.cat([embedded_seq, embedded_cat], dim=2)\n",
        "        output, hidden = self.lstm(combined_embed, hidden)\n",
        "        output = self.fc(output)\n",
        "        return output, hidden\n"
      ],
      "metadata": {
        "id": "HrxP1LO5EcpH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, dataloader, epochs, optimizer, criterion, scheduler, device):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for cat_idx, input_seq, target_seq in dataloader:\n",
        "            cat_idx = cat_idx.to(device)\n",
        "            input_seq = input_seq.to(device)\n",
        "            target_seq = target_seq.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            output, _ = model(cat_idx, input_seq)\n",
        "            output_flat = output.view(-1, output.size(-1))\n",
        "            loss = criterion(output_flat, target_seq.view(-1))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        scheduler.step()\n",
        "        print(f'Epoch {epoch+1}: Loss = {total_loss / len(dataloader)}')\n",
        "\n",
        "# Assuming 'device' and 'model' are defined and properly initialized\n",
        "# Assuming 'dataloader' is an instance of DataLoader using PickupLinesDataset\n"
      ],
      "metadata": {
        "id": "sA8l3_jCEhXq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau\n",
        "categories = {\n",
        "    'normal': normal,\n",
        "    'math': math,\n",
        "    'science': science,\n",
        "    'church': church,\n",
        "    'sleep': sleep,\n",
        "    'knock': knock\n",
        "}\n",
        "\n",
        "dataset = PickupLinesCategoryDataset(categories, chunk_len=10)\n",
        "dataloaderC = DataLoader(dataset, batch_size=128, shuffle=True)\n",
        "\n",
        "vocab_size = len(dataset.vocab)\n",
        "category_count = len(categories)\n",
        "embedding_dim = 256\n",
        "hidden_dim = 512\n",
        "category_embedding_dim = 50\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "modelC = PickupLineCategoryGenerator(vocab_size, embedding_dim, hidden_dim, category_count, category_embedding_dim)\n",
        "modelC.to(device)\n",
        "optimizerC = torch.optim.Adam(modelC.parameters(), lr=0.001)\n",
        "criterionC = nn.CrossEntropyLoss()\n",
        "schedulerC = StepLR(optimizerC, step_size=30, gamma=0.1)\n",
        "trainCategory(modelC, dataloaderC, 100, optimizerC, criterionC, schedulerC, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ylMDk9t6EnC3",
        "outputId": "d4a3cae9-9f06-427c-9726-e34748dc1bda"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 1.6558877647358134\n",
            "Epoch 2, Loss: 1.2721155292672086\n",
            "Epoch 3, Loss: 1.1291076218373335\n",
            "Epoch 4, Loss: 1.0412259604684753\n",
            "Epoch 5, Loss: 0.9816602350400063\n",
            "Epoch 6, Loss: 0.9402606706052428\n",
            "Epoch 7, Loss: 0.910374926327416\n",
            "Epoch 8, Loss: 0.8880857254715483\n",
            "Epoch 9, Loss: 0.8712996635993902\n",
            "Epoch 10, Loss: 0.8579392809813164\n",
            "Epoch 11, Loss: 0.8473520679195432\n",
            "Epoch 12, Loss: 0.8381845768972284\n",
            "Epoch 13, Loss: 0.8309573504425065\n",
            "Epoch 14, Loss: 0.8242782054156282\n",
            "Epoch 15, Loss: 0.8193558243193642\n",
            "Epoch 16, Loss: 0.8142164821918118\n",
            "Epoch 17, Loss: 0.8099778249813195\n",
            "Epoch 18, Loss: 0.8064659871821359\n",
            "Epoch 19, Loss: 0.8032053386456527\n",
            "Epoch 20, Loss: 0.7998407938060224\n",
            "Epoch 21, Loss: 0.7970773475137816\n",
            "Epoch 22, Loss: 0.7943911250366036\n",
            "Epoch 23, Loss: 0.7921434683148381\n",
            "Epoch 24, Loss: 0.7904982312628077\n",
            "Epoch 25, Loss: 0.7881974586605156\n",
            "Epoch 26, Loss: 0.7866294978184546\n",
            "Epoch 27, Loss: 0.7848938127379472\n",
            "Epoch 28, Loss: 0.783250671233574\n",
            "Epoch 29, Loss: 0.7813546456808342\n",
            "Epoch 30, Loss: 0.7802738894138396\n",
            "Epoch 31, Loss: 0.7353675205715008\n",
            "Epoch 32, Loss: 0.7240579987218656\n",
            "Epoch 33, Loss: 0.721126224624228\n",
            "Epoch 34, Loss: 0.7195158018434383\n",
            "Epoch 35, Loss: 0.7184056732279168\n",
            "Epoch 36, Loss: 0.7175267725343873\n",
            "Epoch 37, Loss: 0.7168729947928467\n",
            "Epoch 38, Loss: 0.7162953138475746\n",
            "Epoch 39, Loss: 0.7157253208448789\n",
            "Epoch 40, Loss: 0.7152769466759141\n",
            "Epoch 41, Loss: 0.7148330572747836\n",
            "Epoch 42, Loss: 0.7144016832330801\n",
            "Epoch 43, Loss: 0.714088885204884\n",
            "Epoch 44, Loss: 0.7136886397666056\n",
            "Epoch 45, Loss: 0.7134035611923345\n",
            "Epoch 46, Loss: 0.713125745683318\n",
            "Epoch 47, Loss: 0.7127977078226982\n",
            "Epoch 48, Loss: 0.7125449618781073\n",
            "Epoch 49, Loss: 0.7122978918619524\n",
            "Epoch 50, Loss: 0.7119764721679489\n",
            "Epoch 51, Loss: 0.7117857878349869\n",
            "Epoch 52, Loss: 0.7115804998668317\n",
            "Epoch 53, Loss: 0.7113429003388342\n",
            "Epoch 54, Loss: 0.7111165989675909\n",
            "Epoch 55, Loss: 0.7109037963391842\n",
            "Epoch 56, Loss: 0.7107074059957756\n",
            "Epoch 57, Loss: 0.7105496733355199\n",
            "Epoch 58, Loss: 0.7103802998073406\n",
            "Epoch 59, Loss: 0.7101625596892722\n",
            "Epoch 60, Loss: 0.709981297738112\n",
            "Epoch 61, Loss: 0.702199081186209\n",
            "Epoch 62, Loss: 0.7016676793008949\n",
            "Epoch 63, Loss: 0.7014621146279654\n",
            "Epoch 64, Loss: 0.7013487333550319\n",
            "Epoch 65, Loss: 0.7012626856907317\n",
            "Epoch 66, Loss: 0.7012064745950749\n",
            "Epoch 67, Loss: 0.7011813272152007\n",
            "Epoch 68, Loss: 0.7011465900757266\n",
            "Epoch 69, Loss: 0.701082733239322\n",
            "Epoch 70, Loss: 0.701058532510485\n",
            "Epoch 71, Loss: 0.7010165899215077\n",
            "Epoch 72, Loss: 0.7009950035928562\n",
            "Epoch 73, Loss: 0.7009725039892823\n",
            "Epoch 74, Loss: 0.7009567646338868\n",
            "Epoch 75, Loss: 0.7009225575344654\n",
            "Epoch 76, Loss: 0.7008892587998861\n",
            "Epoch 77, Loss: 0.7008792056281574\n",
            "Epoch 78, Loss: 0.7008458543743654\n",
            "Epoch 79, Loss: 0.7008135257846247\n",
            "Epoch 80, Loss: 0.7008109234421047\n",
            "Epoch 81, Loss: 0.7007810156462215\n",
            "Epoch 82, Loss: 0.700733686512777\n",
            "Epoch 83, Loss: 0.7007282631737846\n",
            "Epoch 84, Loss: 0.700714200517557\n",
            "Epoch 85, Loss: 0.7006819339564246\n",
            "Epoch 86, Loss: 0.7006651957415441\n",
            "Epoch 87, Loss: 0.7006342534725559\n",
            "Epoch 88, Loss: 0.7006104881870361\n",
            "Epoch 89, Loss: 0.7005916718894672\n",
            "Epoch 90, Loss: 0.7005748488980116\n",
            "Epoch 91, Loss: 0.6995314583415408\n",
            "Epoch 92, Loss: 0.6995335042911724\n",
            "Epoch 93, Loss: 0.6995247601841239\n",
            "Epoch 94, Loss: 0.6995304626518544\n",
            "Epoch 95, Loss: 0.6995090923667326\n",
            "Epoch 96, Loss: 0.699515956907501\n",
            "Epoch 97, Loss: 0.6995120977087488\n",
            "Epoch 98, Loss: 0.6995028274772811\n",
            "Epoch 99, Loss: 0.6995352948544794\n",
            "Epoch 100, Loss: 0.6995041699180762\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(model, dataset, category, start_phrase, max_length):\n",
        "    model.eval()\n",
        "    input_idx = [dataset.char_to_int[char] for char in start_phrase]\n",
        "    input_tensor = torch.tensor([input_idx], dtype=torch.long).to(device)\n",
        "    cat_idx = torch.tensor([dataset.cat_to_int[category]], dtype=torch.long).to(device)\n",
        "\n",
        "    output_text = start_phrase\n",
        "    hidden = None\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(max_length):\n",
        "            output, hidden = model(cat_idx, input_tensor, hidden)\n",
        "            probabilities = torch.softmax(output[:, -1, :], dim=-1)\n",
        "            char_idx = torch.multinomial(probabilities, 1).item()\n",
        "            next_char = dataset.int_to_char[char_idx]\n",
        "            output_text += next_char\n",
        "            input_tensor = torch.tensor([[char_idx]], dtype=torch.long).to(device)\n",
        "            if next_char == '\\n':\n",
        "                break\n",
        "\n",
        "    return output_text\n",
        "print(generate(modelC, dataset, 'math', 'You', 150))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "id": "e9h5Tq4eOZit",
        "outputId": "4f3ef2b3-209b-4f6d-bcae-270597232a2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'modelC' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-e3e0b92efa29>\u001b[0m in \u001b[0;36m<cell line: 22>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moutput_text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodelC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'math'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'You'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m150\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'modelC' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau\n",
        "def repackage_hidden(h):\n",
        "  \"\"\"Wraps hidden states in new Tensors, to detach them from their history.\"\"\"\n",
        "  if isinstance(h, torch.Tensor):\n",
        "    return h.detach()\n",
        "  else:\n",
        "    return tuple(repackage_hidden(v) for v in h)\n",
        "\n",
        "def train(model, dataloader, epochs, optimizer, criterion, scheduler, device):\n",
        "  model.train()\n",
        "  for epoch in range(epochs):\n",
        "\n",
        "    #running_loss = 0.0\n",
        "    #hidden = model.init_hidden(dataloader.batch_size)\n",
        "    total_loss = 0\n",
        "    for inputs, targets in dataloader:\n",
        "      inputs, targets = inputs.to(device), targets.to(device)\n",
        "      optimizer.zero_grad()\n",
        "      batch_size = inputs.size(0)\n",
        "      hidden = model.init_hidden(batch_size)\n",
        "\n",
        "      output, hidden = model(inputs, hidden)\n",
        "      loss = criterion(output, targets.view(-1))\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      total_loss += loss.item()\n",
        "      # running_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "    print(f'Epoch {epoch+1}, Avg Loss: {total_loss / len(dataloader)}')\n",
        "    scheduler.step()\n",
        "    # print(f'Epoch {epoch+1}, Loss: {running_loss/len(dataloader.dataset)}')\n",
        "    # validate_model(model, valid_loader, criterion, device)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = PickupLineGenerator(len(dataset.chars), 256, 512, 2)\n",
        "model.to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "scheduler = StepLR(optimizer, step_size=30, gamma=0.1)\n",
        "# Alternatively, use ReduceLROnPlateau for more dynamic scheduling\n",
        "# scheduler = ReduceLROnPlateau(optimizer, 'min', patience=10, factor=0.1, verbose=True)\n",
        "train(model, dataloader, 25, optimizer, criterion, scheduler, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TqxB6H0NtEmY",
        "outputId": "c90931a9-e9d4-4874-ef91-a6d23e5ad45d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Avg Loss: 2.478307977089515\n",
            "Epoch 2, Avg Loss: 1.585894327897292\n",
            "Epoch 3, Avg Loss: 1.36457934654676\n",
            "Epoch 4, Avg Loss: 1.2004481013004595\n",
            "Epoch 5, Avg Loss: 1.0720143813353318\n",
            "Epoch 6, Avg Loss: 0.9708557220605704\n",
            "Epoch 7, Avg Loss: 0.8826700494839594\n",
            "Epoch 8, Avg Loss: 0.7977023619871874\n",
            "Epoch 9, Avg Loss: 0.7329606890678406\n",
            "Epoch 10, Avg Loss: 0.6596093980165628\n",
            "Epoch 11, Avg Loss: 0.6053705050395085\n",
            "Epoch 12, Avg Loss: 0.5573770330502437\n",
            "Epoch 13, Avg Loss: 0.5118330996770125\n",
            "Epoch 14, Avg Loss: 0.46768747659829946\n",
            "Epoch 15, Avg Loss: 0.43860442088200496\n",
            "Epoch 16, Avg Loss: 0.4063060992039167\n",
            "Epoch 17, Avg Loss: 0.3541602473992568\n",
            "Epoch 18, Avg Loss: 0.3367928030399176\n",
            "Epoch 19, Avg Loss: 0.31293208576165715\n",
            "Epoch 20, Avg Loss: 0.280379421206621\n",
            "Epoch 21, Avg Loss: 0.27380289916808787\n",
            "Epoch 22, Avg Loss: 0.24839233595591326\n",
            "Epoch 23, Avg Loss: 0.23080607956418625\n",
            "Epoch 24, Avg Loss: 0.21970850309500328\n",
            "Epoch 25, Avg Loss: 0.2166504483956557\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_categorical_pickup_line(model, category, start_phrase, max_length):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    device = next(model.parameters()).device  # Ensure the tensor is on the same device as the model\n",
        "\n",
        "    # Convert start_phrase to tensor\n",
        "    input_seq = torch.tensor([[dataset.char_to_int[ch] for ch in start_phrase]], dtype=torch.long).to(device)\n",
        "    category_tensor = torch.tensor([dataset.cat_to_int[category]], dtype=torch.long).to(device)\n",
        "\n",
        "    output_line = start_phrase\n",
        "\n",
        "    hidden = None  # Initialize hidden state (if your model uses one and needs manual initialization)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(max_length):\n",
        "            output, hidden = model(category_tensor, input_seq, hidden)\n",
        "            # Take the last time step from the last layer of the LSTM output\n",
        "            probabilities = torch.softmax(output[0, -1], dim=-1)\n",
        "            char_idx = torch.multinomial(probabilities, 1).item()\n",
        "            next_char = dataset.int_to_char[char_idx]  # Correct dictionary for character lookup\n",
        "\n",
        "            output_line += next_char\n",
        "            # Prepare the next input to be the character just generated\n",
        "            input_seq = torch.tensor([[char_idx]], dtype=torch.long).to(device)\n",
        "\n",
        "            if next_char == '\\n':  # Optionally: stop if the model generates a newline character\n",
        "                break\n",
        "\n",
        "    return output_line\n",
        "\n",
        "\n",
        "print(generate_categorical_pickup_line(modelC, 'normal', 'You must be ', 50))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "id": "MbUULgAIMTNj",
        "outputId": "4bc3a4d6-c324-4769-d7ab-47966dd0147b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'PickupLinesCategoryDataset' object has no attribute 'int_to_char'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-f72fb380a9fb>\u001b[0m in \u001b[0;36m<cell line: 31>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerate_categorical_pickup_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodelC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'normal'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'You must be '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-24-f72fb380a9fb>\u001b[0m in \u001b[0;36mgenerate_categorical_pickup_line\u001b[0;34m(model, category, start_phrase, max_length)\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mprobabilities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mchar_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultinomial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobabilities\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m             \u001b[0mnext_char\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint_to_char\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchar_idx\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Correct dictionary for character lookup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0moutput_line\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnext_char\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'PickupLinesCategoryDataset' object has no attribute 'int_to_char'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PickupLineGenerator(nn.Module):\n",
        "  def __init__(self, vocab_size, embedding_dim, hidden_dim, n_layers, dropout=0.5):\n",
        "    super(PickupLineGenerator, self).__init__()\n",
        "    self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "    self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=1, batch_first=True)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.lstm1 = nn.LSTM(hidden_dim, hidden_dim, num_layers=1, batch_first=True)\n",
        "    self.dropout1 = nn.Dropout(dropout)\n",
        "    #self.layer_norm = nn.LayerNorm(hidden_dim)\n",
        "    self.fc = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "  def forward(self, x, hidden):\n",
        "    embedded = self.embedding(x)\n",
        "    output, hidden0 = self.lstm(embedded, hidden[0])\n",
        "    output = self.dropout(output)\n",
        "    output, hidden1 = self.lstm1(output, hidden[1])\n",
        "    output = self.dropout1(output)\n",
        "    #output = self.layer_norm(output[:, -1, :])\n",
        "    output = self.fc(output[:, -1, :])\n",
        "    return output, (hidden0, hidden1)\n",
        "\n",
        "  def init_hidden(self, batch_size):\n",
        "    weight = next(self.parameters()).data\n",
        "    #hidden = (weight.new(self.lstm.num_layers, batch_size, self.lstm.hidden_size).zero_(),\n",
        "             #weight.new(self.lstm.num_layers, batch_size, self.lstm.hidden_size).zero_())\n",
        "    hidden = (\n",
        "            (weight.new_zeros(1, batch_size, self.lstm.hidden_size),\n",
        "             weight.new_zeros(1, batch_size, self.lstm.hidden_size)),\n",
        "            (weight.new_zeros(1, batch_size, self.lstm1.hidden_size),\n",
        "             weight.new_zeros(1, batch_size, self.lstm1.hidden_size))\n",
        "        )\n",
        "    return hidden"
      ],
      "metadata": {
        "id": "HMIT2SWOrv-M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "def generate(model, start_str, len_gen, temperature=1.0):\n",
        "    model.eval()\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    hidden = model.init_hidden(1)\n",
        "    input_seq = torch.tensor([[dataset.char_to_int[ch] for ch in start_str]], dtype=torch.long).to(device)\n",
        "    predicted = start_str\n",
        "\n",
        "    for _ in range(len_gen):\n",
        "        output, hidden = model(input_seq, hidden)\n",
        "        output_dist = F.softmax(output.data.view(-1) / temperature, dim=0)\n",
        "        # output_dist = output.data.view(-1).div(temperature).exp()\n",
        "        top_char = torch.multinomial(output_dist, 1)[0]\n",
        "        predicted_char = dataset.int_to_char[top_char.item()]\n",
        "        predicted += predicted_char\n",
        "        input_seq = torch.tensor([[top_char]], dtype=torch.long).to(device)\n",
        "\n",
        "    return predicted\n",
        "\n",
        "\n",
        "# Generate a new pickup line\n",
        "print(generate(model, 'Knock', 100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E7WIuxwEwxC9",
        "outputId": "cdfa3054-6c39-4b4b-f0ca-d1ec63d2555b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Knock Knock Who's there? Mirra. Mirra who? Howard you like a big kiss?\n",
            "Knock Knock Who's there? Mistleter\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SOGC1le3x659",
        "outputId": "a20a69c8-b90b-4530-c89c-a9d37cf502b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: unidecode in /usr/local/lib/python3.10/dist-packages (1.3.8)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.2.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.4.127)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "# We getting all the characters here\n",
        "! pip install unidecode\n",
        "! pip install torch\n",
        "\n",
        "import unidecode\n",
        "import string\n",
        "import random\n",
        "\n",
        "additional_characters = ['’', '…', '—', '“', '”']\n",
        "all_characters = string.printable + ''.join(additional_characters)\n",
        "n_characters = len(all_characters)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "chunk_len = 100\n",
        "X = []\n",
        "Y = []\n",
        "for i in range(0, n_chars - chunk_len, 1):\n",
        "  inp = all_lines[i:i+chunk_len]\n",
        "  out = all_lines[i+chunk_len]\n",
        "  X.append([chars_to_int[char] for char in inp])\n",
        "  Y.append([chars_to_int[out]])\n",
        "n_patterns = len(X)\n",
        "\n",
        "X = np.reshape(X, (n_patterns, chunk_len, 1))\n",
        "X = X/float(n_vocab)\n",
        "Y = F.one_hot(torch.tensor(Y))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eB4Mjz-0lrVh",
        "outputId": "8031a3fe-7b9e-45db-9452-2386f5542f77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[0, 1, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(Y.size())\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SdwHovw6pXve",
        "outputId": "17c117ce-2e52-4136-da0f-2a45d88405b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([414317, 1, 82])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TBnJuWi37dX5"
      },
      "outputs": [],
      "source": [
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "\n",
        "tokenizer = get_tokenizer('basic_english')\n",
        "def yield_tokens(data_iter):\n",
        "    for text in data_iter:\n",
        "        yield tokenizer(text)\n",
        "\n",
        "vocab = build_vocab_from_iterator(yield_tokens(pickup_lines), specials=[\"<unk>\", \"<pad>\"])\n",
        "vocab.set_default_index(vocab[\"<unk>\"])  # default index for unknown words\n",
        "\n",
        "# Numericalizing text\n",
        "text_pipeline = lambda x: [vocab[token] for token in tokenizer(x)]\n",
        "\n",
        "# Convert texts to padded sequence of indices\n",
        "max_length = min(len(text_pipeline(line)) for line in pickup_lines)\n",
        "print(max_length)\n",
        "padded_sequences = [text_pipeline(line) + [vocab['<pad>']] * (max_length - len(text_pipeline(line))) for line in pickup_lines]\n",
        "\n",
        "# Convert to tensor\n",
        "import torch\n",
        "prepped_data = torch.tensor(padded_sequences, dtype=torch.long)\n",
        "input_seq = prepped_data[:, :-1]\n",
        "target_seq = prepped_data[:, 1:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CmbyCBlcyUw-",
        "outputId": "6a4ff0d8-12d8-4706-a5e8-f830441329f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Baby you thicker than a novel, and I wanna read all yo pagesIf looks could kill you would be a weapon of mass destruction.Here, let me chalk up those jugs for you.Are you the sun? Because you're the c\n"
          ]
        }
      ],
      "source": [
        "# Here well get a random chunk\n",
        "import random\n",
        "\n",
        "chunk_len = 200\n",
        "\n",
        "\n",
        "def random_chunk():\n",
        "  chunk = ''\n",
        "  while len(chunk) < chunk_len:\n",
        "    i = random.randint(0, df.size)\n",
        "    if len(pickup_lines[i]) + len(chunk) < chunk_len:\n",
        "      chunk += pickup_lines[i]\n",
        "      i += 1\n",
        "    else:\n",
        "      remaining = chunk_len - len(chunk)\n",
        "      chunk += pickup_lines[i][:remaining]\n",
        "  return chunk\n",
        "\n",
        "print(random_chunk())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jA5rNKcIymRV",
        "outputId": "49849e05-0c1f-438b-f3a7-c2e7f97f3310"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([10, 11, 12, 39, 40, 41])\n"
          ]
        }
      ],
      "source": [
        "# Idk what the fuck this does but its dope\n",
        "import torch\n",
        "char_to_index = {ch: idx for idx, ch in enumerate(all_characters)}\n",
        "# Turn string into list of longs\n",
        "def char_tensor(string):\n",
        "  tensor = torch.zeros(len(string), dytpe=torch.long)\n",
        "  for c in range(len(string)):\n",
        "    tensor[c] = char_to_index.get(string[c], 0)\n",
        "\n",
        "  return tensor\n",
        "\n",
        "print(char_tensor('abcDEF'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "70LDjp0ry_yx"
      },
      "outputs": [],
      "source": [
        "# Some more shit that's dope\n",
        "def random_training_set():\n",
        "  chunk = random_chunk()\n",
        "  inp = char_tensor(chunk[:-1])\n",
        "  target = char_tensor(chunk[1:])\n",
        "  return inp, target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PBR82dgazBzz"
      },
      "outputs": [],
      "source": [
        "# NICE an RNN\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "class RNN(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, output_size, n_layers=1):\n",
        "    super(RNN, self).__init__()\n",
        "    self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "    self.n_layers = n_layers\n",
        "    self.hidden_size = hidden_size\n",
        "    self.relu = nn.ReLU()\n",
        "    self.gru = nn.GRU(hidden_size, hidden_size, n_layers)\n",
        "    self.fc = nn.Linear(hidden_size, output_size)\n",
        "    self.output_size = output_size\n",
        "\n",
        "  def forward(self, input, hidden):\n",
        "    embedded = self.embedding(input).view(1, 1, -1)\n",
        "    output, hidden = self.gru(embedded, hidden)\n",
        "    output = self.fc(output)\n",
        "    out_decoded = self.relu(output)\n",
        "\n",
        "    return out_decoded, hidden\n",
        "\n",
        "  def init_hidden(self):\n",
        "    return torch.zeros(self.n_layers, 1, self.hidden_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ugwG22-3zICO"
      },
      "outputs": [],
      "source": [
        "# Train this shit\n",
        "# NOTE: decoder_optimizer, decoder, and criterion will be defined below as global variables\n",
        "def train(inp, target):\n",
        "  decoder_optimizer.zero_grad()\n",
        "  loss = 0\n",
        "  hidden = decoder.init_hidden()\n",
        "  for i, input in enumerate(inp):\n",
        "    output, hidden = decoder(input, hidden)\n",
        "    loss += criterion(output.squeeze(0), target[i].unsqueeze(0))\n",
        "  loss.backward()\n",
        "  decoder_optimizer.step()\n",
        "\n",
        "  return loss.item()\n",
        "  \"\"\"criterion = nn.CrossEntropyLoss()\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
        "  inp = inp.unsqueeze(1)\n",
        "  for epoch in range(num_epochs):\n",
        "    total_loss = 0\n",
        "    model.train()\n",
        "    for i in range (0, inp.size(0), batch_size):\n",
        "      hidden = model.init_hidden()\n",
        "      optimizer.zero_grad()\n",
        "      inputs = inp[i:i+batch_size]\n",
        "      targets = target[i:i+batch_size]\n",
        "\n",
        "      outputs, hidden = model(inputs, hidden)\n",
        "      loss = criterion(outputs.view(-1, model.output_size), targets.view(-1))\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      total_loss += loss.item()\n",
        "\n",
        "    print('Epoch: {}/{}.............'.format(epoch+1, num_epochs), end=' ')\n",
        "    print(\"Loss: {:.4f}\".format(total_loss / inp.size(0)))\"\"\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uCmOJwEDzJ8Q"
      },
      "outputs": [],
      "source": [
        "# Sample output fuck shit\n",
        "def sample_outputs(output, temperature):\n",
        "    \"\"\"Takes in a vector of unnormalized probability weights and samples a character from the distribution\"\"\"\n",
        "    # As temperature approaches 0, this sampling function becomes argmax (no randomness)\n",
        "    # As temperature approaches infinity, this sampling function becomes a purely random choice\n",
        "    return torch.multinomial(torch.exp(output / temperature), 1)\n",
        "\n",
        "def evaluate(prime_str='A', predict_len=100, temperature=0.8):\n",
        "  ## initialize hidden state, initialize other useful variables\n",
        "    # your code here\n",
        "  ## /\n",
        "  hidden_state = decoder.init_hidden()\n",
        "  input = char_tensor(prime_str)[-1]\n",
        "  generated_text = [prime_str]\n",
        "  with torch.no_grad():\n",
        "    for i in range(predict_len):\n",
        "      output, hidden_state = decoder(input, hidden_state)\n",
        "      sample_output = sample_outputs(output.squeeze(0), temperature)\n",
        "      next_char = all_characters[sample_output.item()]\n",
        "      generated_text.append(next_char)\n",
        "      input = char_tensor(next_char)\n",
        "\n",
        "\n",
        "  return ''.join(generated_text)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1m5P2ZvhzUHH"
      },
      "outputs": [],
      "source": [
        "# Set up the model bitch\n",
        "import time\n",
        "n_epochs = 2000\n",
        "print_every = 200\n",
        "plot_every = 10\n",
        "hidden_size = 200\n",
        "n_layers = 3\n",
        "lr = 0.001\n",
        "decoder = nn.Sequential()\n",
        "decoder.Add(nn.LTSM(256, input_shape=()))\n",
        "#decoder = RNN(n_characters, hidden_size, n_characters, n_layers)\n",
        "#train(model, input_seq, target_seq, n_epochs)\n",
        "decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=lr)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# optimizer = torch.optim.Adam(decoder.parameters(), lr=lr)\n",
        "start = time.time()\n",
        "all_losses = []\n",
        "loss_avg = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mYWvFEzhzWQB",
        "outputId": "a53ec0fb-3168-47ca-e103-57f350526ac8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[120.60692834854126 (200 10%) 520.1180]\n",
            "Wh/Pn.. /ror thire a f9eDNas a hout I anghrarnase got me got od the beat \\an mauslet in] nouls a a but \n",
            "\n",
            "[234.40011167526245 (400 20%) 579.9518]\n",
            "Wh!'-\u000bs hot are the tonight \to deing a I don lood ond ithing to here of mabe# a jide fise …eer.I me th \n",
            "\n",
            "[367.22923731803894 (600 30%) 433.1595]\n",
            "Wh?9’“\\5-9QF\u000bu= meive a the siving in intiis a dindart, let is I beaut from hought.Ca* RoTnoU mapZ mab \n",
            "\n",
            "[482.3747613430023 (800 40%) 464.2769]\n",
            "Wh+^re the get ^rath it and ~ater some.Fore his tose 791 Gom.,I donejVber sart sas more Me , that dene \n",
            "\n",
            "[594.611447095871 (1000 50%) 484.6093]\n",
            "Wh\\…Q*Trite $(7{ for line <oz of the hould life.I PW—'le the see \fisquint out is of to mineges.7Ro! Qn \n",
            "\n",
            "[707.1234545707703 (1200 60%) 403.3609]\n",
            "Wh.[what \n",
            "omestest …as reall ammar—ing shat is the name Qreall! % ?Can ?I or 6a[ the onl6 me into go a \n",
            "\n",
            "[818.8365364074707 (1400 70%) 334.5836]\n",
            "Wher some an an outruate in \u000bul”n our not a smaling out out to see ous for some onl, \fuss love the fis \n",
            "\n",
            "[926.8016941547394 (1600 80%) 373.4211]\n",
            "Wht heart hot be a bo< eOsing the same and be greating me on pans isnce hands hear. I danre me the dam \n",
            "\n",
            "[1035.9735019207 (1800 90%) 380.1441]\n",
            "Whered looking that and into Zeagangle.I -not hot piking are reall* s\"y that Countanders thome miss an \n",
            "\n",
            "[1147.9865138530731 (2000 100%) 380.7031]\n",
            "WhRole ormaps, I have in the s\tister to pined : to plake of sodiGo haid a happerCall\" I do?U< ! = pumb \n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Run this shit\n",
        "# n_epochs = 2000\n",
        "for epoch in range(1, n_epochs + 1):\n",
        "  loss_ = train(*random_training_set())\n",
        "  loss_avg += loss_\n",
        "\n",
        "  if epoch % print_every == 0:\n",
        "      print('[%s (%d %d%%) %.4f]' % (time.time() - start, epoch, epoch / n_epochs * 100, loss_))\n",
        "      print(evaluate('Wh', 100), '\\n')\n",
        "\n",
        "  if epoch % plot_every == 0:\n",
        "      all_losses.append(loss_avg / plot_every)\n",
        "      loss_avg = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a6KBgFQFzZi9"
      },
      "outputs": [],
      "source": [
        "for i in range(10):\n",
        "  start_strings = [\" Th\", \" wh\", \" he\", \" I \", \" ca\", \" G\", \" lo\", \" ra\"]\n",
        "  start = random.randint(0,len(start_strings)-1)\n",
        "  print(start_strings[start])\n",
        "#   all_characters.index(string[c])\n",
        "  print(evaluate(start_strings[start], 200), '\\n')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "id": "sDsqDZ4aIQWt",
        "outputId": "36ea7d73-7d09-4865-e60f-76ad6fac6915"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-53-c97a9efd5ec8>\u001b[0m in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnew_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerate_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_phrase\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Are you a\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgeneration_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m75\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ],
      "source": [
        "def generate_line(model, start_phrase=\"Hello\", generation_length=75):\n",
        "  model.eval()\n",
        "  words = start_phrase.split()\n",
        "  state = model.init_hidden()\n",
        "  for w in words[:-1]:\n",
        "    idx = torch.tensor([[vocab[w]]], dtype=torch.long)\n",
        "    output, state = model(idx, state)\n",
        "  new_words = []\n",
        "  next_word = words[-1]\n",
        "  for _ in range(generation_length):\n",
        "    idx = torch.tensor([[vocab[next_word]]], dtype=torch.long)\n",
        "    output, state = model(idx, state)\n",
        "    next_word = output.squeeze(0).squeeze(0).argmax().item()\n",
        "    next_word = vocab.lookup_token(next_word)\n",
        "    new_words.append(next_word)\n",
        "  return ' '.join(words + new_words)\n",
        "\n",
        "print(generate_line(model, start_phrase=\"Are you a\", generation_length=75))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}